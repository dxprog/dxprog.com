<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Archives - Page 10</title><link rel="stylesheet" type="text/css" href="http://staticr.dxprog.com/static/css/index.css"/></head><body><section class="rollup-page"><section class="intro-bar"><img src="http://staticr.dxprog.com/static/images/me.jpg" alt="Matt Hackmann" class="intro-bar__photo"/><h1 class="intro-bar__header"><span class="intro-bar__name intro-bar__name--first">Matt</span><span class="intro-bar__name intro-bar__name--last">Hackmann</span></h1><h2 class="intro-bar__subhead">The thoughts and goings-on of some programmer dude.</h2><nav class="intro-bar__social-nav"><ul class="social-links"><li class="social-links__item"><a class="social-links__link social-links__link--rss" target="_blank" href="http://feeds.feedburner.com/dxprog">RSS Feed</a></li><li class="social-links__item"><a class="social-links__link social-links__link--github" target="_blank" href="https://github.com/dxprog">GitHub</a></li><li class="social-links__item"><a class="social-links__link social-links__link--twitter" target="_blank" href="https://twitter.com/dxprog">Twitter</a></li><li class="social-links__item"><a class="social-links__link social-links__link--linkedin" target="_blank" href="https://www.linkedin.com/in/mhackmann">LinkedIn Profile</a></li></ul></nav></section><section class="article-list"><article class="post"><header class="post__header"><h1 class="post__title"><a href="http://staticr.dxprog.com/entry/getting-around-the-iron-curtains">Getting Around the Iron Curtains</a></h1><time class="post__published">February 23, 2014</time></header><div class="post__content"><p>Last week, a friend clued me into CloudFlare, a CDN service that also somehow has a free tier with unlimited bandwidth. This played right into my fears of hitting the 4TB bandwidth ceiling on my current hosting tier. Put into perspective, that&#39;s pretty close to all the digital storage I have available in my apartment. Also, it quells the fears that users on the other side of the pond are getting subpar download times. So, with minimal weighing of the consequences, I flipped everything over. And, lo and behold, it worked!</p>
<p>Almost...</p>
<p>You see, some countries and ISPs think that blocking shit on the internet is a really great idea. I&#39;m not going to go on a political rampage right now because that&#39;s not my style, but when you&#39;re getting messages about users unable to download content from your hosting due to these reasons, it&#39;s more than a little irritating. But, goddammit, I&#39;m a programmer and there had to be a solution that&#39;d get me the best of both worlds without shelling out additional money.</p>
<p>After disabling the caching layer on that subdomain (cdn.awwni.me), I began weighing my options. Somehow I needed to find out if a particular user had the ability to talk to CloudFlare (now working through licdn.awwni.me). Most views were going to be served straight out of reddit.com, but I had a couple of attack vectors: the redditbooru.com main site, which is hit often for people doing repost checks or looking at albums that have been posted to reddit, and the RedditBooru browser extensions. The former was going to be the easiest to tinker with (so I thought) and affect the most people.</p>
<p>Since I had a way to test availability, I needed to actually perform the test itself. What I devised was an iframe that would be loaded on every page of RedditBooru. It would load the following script:</p>
<pre><code class="lang-html">&lt;html&gt;
    &lt;head&gt;
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://licdn.awwni.me/cdncheck.js&quot;&gt;&lt;/script&gt;
    &lt;/head&gt;
&lt;/html&gt;
</code></pre>
<pre><code class="lang-javascript">// If this page has managed to load, go ahead and set the CDN cookie to expire on my 100th birthday
var date = (new Date(&#39;2086/6/8&#39;)).toGMTString();
document.cookie = &#39;use_cdn=true; expires=&#39; + date;
</code></pre>
<p>If the user has access to the licdm subdomain, a cookie is set noting such and does so on the cdn.awwni.me domain. Since these users were now marked, this information can be used in nginx to reroute the user.</p>
<pre><code class="lang-nginx">if ($host != licdn.awwni.me) {
    set $useCdn &quot;P&quot;;
}

    if ($http_cookie ~* &#39;use_cdn&#39;) {
    set $useCdn &quot;${useCdn}C&quot;;
}

if ($useCdn = PC) {
            rewrite ^/(.*) http://licdn.awwni.me/$1 permanent;
}
</code></pre>
<p>Getting around nginx&#39; inability to do multiple conditionals in one line aside, the above checks to make sure that the cookie is set and that the user is <em>not</em> coming from the licdn.awwni.me domain. That last bit was added when I accidentally set the CDN cookie on licdn and got caught in an infinite redirect loop for a while. If either of the conditions aren&#39;t met, the image will just serve out of my machine.</p>
<p>There are a few drawbacks to this approach. First of all, every request still hits my machine, so I&#39;m still taking a bandwidth hit. Secondly, those getting served out of the CDN have an extra hop for the redirect. However, I believe neither of these issues are bad enough to warrant removing it completely. For starters, on my side I&#39;m now only serving a handful of bytes as opposed to millions. Secondly, a redirect on a single image is pretty much inconsequential to the user&#39;s experience.</p>
<p>I&#39;ve only been running with this solution for a few hours, so I don&#39;t yet have a good look on how great the benefits are. Time will tell on that one, but I haven&#39;t yet heard any issues on images not loading, so there&#39;s some comfort.</p>
</div></article><article class="post"><header class="post__header"><h1 class="post__title"><a href="http://staticr.dxprog.com/entry/my-god-i-am-a-creepy-person">My God, I am a creepy person</a></h1><time class="post__published">February 15, 2014</time></header><div class="post__content"><p>I have this personal website, which essentially means I am my own little mini NSA. If I wanted to, for example, find out if a person I have very few details on were to visit here, I&#39;d be able to find that out. Let&#39;s say, for example, I know what probable day they&#39;d view this blog, what page they&#39;d be coming from, and approximately where they live.</p>
<p>Well, I have enough data to know (probably) if that person hit my site and what they did when they got here.</p>
<p>As a hypothetical example, let&#39;s say I know they&#39;re somewhere in Northern California, probably came to the homepage via a Google search, and did so on February 13th. Alright, first thing&#39;s first, grep the logs for that day, that page, and exclude crawlers...</p>
<p>Sweet, now I have a list of IPs. Let&#39;s plug those into <a href="http://www.maxmind.com/en/geoip_demo">MaxMind geoip lookup</a> and see what comes of it.</p>
<p>Hmm, no exact match, but ISPs are fuzzy about location and this one is certainly within the area! Alright, now grep the logs for that IP address...</p>
<p>Okay, the person was on the homepage for about three seconds before going to the art page. Once there, they clicked on <a href="http://dxprog.com/uploads/Vesperia-Doll.jpg">three</a> <a href="http://dxprog.com/uploads/Kagami_Doll.jpg">pieces</a> of <a href="http://dxprog.com/uploads/HanakoChristmas.jpg">art</a> before leaving the site. All in all, about a minute of time elapsed between the first and final requests.</p>
<p>Other fun things you can find out while grepping logs is that your mother and former boss are the most frequent visitors to your site. The latter - going under a pseudonym - even listened to the Inception soundtrack on your music page sometime back in November while at work.</p>
<p>This is, of course, all hypothetical.</p>
</div></article><article class="post"><header class="post__header"><h1 class="post__title"><a href="http://staticr.dxprog.com/entry/systems-architecture-in-the-unknown">Systems Architecture in the Unknown</a></h1><time class="post__published">January 29, 2014</time></header><div class="post__content"><p>Just over a year ago, I launched <a href="http://redditbooru.com">redditbooru</a> to the world, rolling together a bunch of ideas spawned by the <a href="http://dxprog.com/entry/a-cute-world-of-programming-possibilities/">awwnime repost checker</a> into a single place. In addition to that, I added the ability for users to host content in addition to being able to index multiple subreddits. To say that it became a wild success would almost be selling it short. At launch, it was indexing maybe 5 subs, now it&#39;s 16, my server went from averaging ~23/Kbps per month to over 10/Mbps, and am nearing 100k pageviews for a rolling 30 day period. Despite exponential growth in traffic, the site has experienced almost no traffic related issues proving that my code is able to handle scale. What started off as a fun little afternoon of experimenting has become quite a success.</p>
<p>However, things are not without their flaws. On the technical side, the cron that does the indexing has some issues; items hosted through redditbooru can&#39;t be posted to more than one sub and indexed in both places and sometimes, in an attempt to pick up on items removed by mods, posts will be incorrectly marked as hidden. On the front end, the user experience is a bit of a mess as I kept bolting on features. In typical Matt Hackmann fashion, I decided that I would rework the entire thing mostly from the ground up with all of my learnings in mind while adding in some user requests. But, I was going to eschew the tried and true PHP/MySQL setup for nodejs/MySQL/mongodb in an attempt to learn something new and bring myself up to speed with the current development fad.</p>
<p>Two weeks into development, I am questioning that decision.[break]</p>
<h2 id="nodejs-and-asynchronous-programming">nodejs and Asynchronous Programming</h2>
<p>Outside of writing some standard libs, the first thing to get running was the indexer. In the past, <a href="https://github.com/dxprog/reddit-booru/blob/master/cron_reddit-booru.php">this was a PHP script run on cron</a> every five minutes or so. One great thing about node is that it&#39;s incredibly simple to write a program that can handle this kind of time management itself. Also, it can do a great many things at once due to its push for making all the things asynchronous. My goal was to make the new indexer able to update all of the subs once per minute and, for the most part, I&#39;ve achieved that. But not without issues along the way.</p>
<p>Say you have 16 different sources you need to get data for once per minute. Currently, you have 16 async ops queued. Now, when each of those resolves, you&#39;re getting 100 posts back from reddit and each of those items is going to need to be acted on, either to create a new post entry or update stats about an old one. Okay, 1600 async ops. If the post is new you&#39;re going to need to do the following:</p>
<p>[list][item]need to make an entry in MySQL with all the post data. asyncOps += 1[/item]
[item]determine where the source data is being hosted. If it&#39;s an image, cool. Go to step 4[/item]
[item]if it&#39;s not an image and, say, an album on imgur, we need to resolve that to a list of images. asyncOps += 1[/item]
[item]we know what images to get, so now we need to retrieve them. this can be done either by external request or a cached version in a mongo store. Either way, asyncOps += numberOfImages[/item]
[item]once the image has downloaded, cached (which can add yet another asyncOp), and processed, we need to create a row for it in MySQL. asyncOps += 1[/item]
[item]once the above operation has comeback with an image ID, we need to save a local copy of the image for serving (asyncOps += 1) and also save a copy to an Amazon S3 store for archival (asyncOp += 1)[/item]
[item]assuming all of that was successful, we need to associate the image to the post, creating yet another row in another table. asyncOps += 1[/item]
[item]assuming all of the above was successful, an object is created from the post and image data and stored in mongo for consumption by the frontend. asyncOps += 1[/item][/list]</p>
<p>So, at an absolute minimum, that&#39;s 7 asynchronous operations per new post. Remember, there could very well be 1600 of these, all that need to be resolved within a minute and the whole operation starts all over again. But, when you have so many waiting operations, things start to fall through the cracks; HTTP calls stop resolving, MySQL calls are either never made or just never heard from again. It&#39;s a bad situation. To get around that problem, I cobbled together a queue system. Reddit requests and post creation/updates are tossed in a big array which is processed every tenth of a second. Added to that, every time an async request is made, a counter is incremented so that the queue system will stop processing once there are so many active async operations. Were I to set that number to one, effectively I&#39;ve just gone back to synchronous code. I initially set the limiter to 100, though due to a problem I will discuss in just a moment, brought it down to 50. And even then, there are issues. But, for the most part, everything updates within the 1 minute time period and hums along nicely. Until...</p>
<h2 id="asynchronous-programming-and-memory-management">Asynchronous Programming and Memory Management</h2>
<p>Say you have a program that analyzes images on a per pixel basis. That requires an incredible amount of memory. redditbooru&#39;s largest hosted image is in the ballpark of 9000x6000 and when you expand that out into 4-byte uncompressed RGBA values, that&#39;s a whopping 206MB. Now, that&#39;s an extreme case; the average image size is 1041x1065, or 4.3MB of uncompressed data. But, since we can have up to 50 things going at once, there&#39;s a potential for 215MB of image data floating around on average. Toss in some of those larger images and you could quite easily exceed the amount of total memory in the system. I&#39;ve been leaving the indexer running for the last several days to ensure its stability, but I came home today to find out it had irrecoverably crashed at some point because it simply ran out of memory. You could have a separate process ensuring that the indexer is always running (similar to <a href="http://dxprog.com/entry/the-plate-and-the-amount-of-stuff-on-it/">what I&#39;ve got implemented in production</a>), but an errant process gobbling up memory is going to slow everything else down, and when you&#39;ve only got the one machine, things need to be as lean as possible.</p>
<p>At this point, I&#39;m at a loss as to how to proceed on that issue. There are a few options:</p>
<p>[list][item]Decrease the queue limiter again[/item]
[item]Add a similar queue to the image processor so that it only runs[/item]
[item]Rearchitect the indexer to run as individual processes and then have a master program spawn a handful of them as needed[/item]
[item]Write an image processor in some respectable language (C++) that does all the image processing[/item]
[item]Go back to PHP[/item][/list]</p>
<h2 id="mongodb-i-don-t-even-need-to-say-anything-else">mongodb - I don&#39;t even need to say anything else</h2>
<p>I&#39;ve been slowly testing the waters with mongo. The concept is interesting, and the promise of running at &quot;web scale&quot; is certainly alluring. However, mongo&#39;s been getting all sorts of bad press and the golden child of the NoSQL era seems to be losing popularity. Still, for my need for having denormalized data, the query mechanism that mongo provides, and a (perceived) notion that it&#39;s supposed to be very good in a read heavy environment, I thought I&#39;d give it a try. Hell, I was even going to see if I could avoid the need for memcache. However, because the front end exclusively relies on this dataset, it needs to reflect the actual database very closely. But, as I found out when testing the image search endpoint, for a given set of image IDs coming back from MySQL, one image was missing from mongo&#39;s store causing javascript&#39;s equivalent of a null pointer exception. Confused, I checked the number of items in MySQL versus the number in mongo.</p>
<p>8% of all images were not accounted for in mongo.</p>
<p>Going back to the list of stuff the cron does above, we know that the very last thing to happen on image creation is the store to mongo. The image has been downloaded, processed, synced to MySQL, saved to disk, and uploaded to AWS. I would expect any one of these to fail before simply inserting an simple object into mongo, something that should never fail because it has no constraint checks. But for 8% of all these inserts to simply... vanish on reported success, it&#39;s like all those warnings I&#39;d read about mongo coming true before my eyes. Luckily, I saw this failure well before it ever hit production. I guess what I&#39;ll do here is move the mongo schema to MySQL and reinstate the memcache layer or maybe even write some internal caching in the js side itself. Or I could forgo that schema entirely and just live with the four or five joins needed to make that data happen. Mongo I&#39;ll keep around for analytics dumping, user sessions, and downloaded image cache, but never again will I consider it for client facing data.</p>
<h2 id="in-summation">In Summation</h2>
<p>Given the two weeks I&#39;ve been working on this and the problems I&#39;ve had already (and the problems still outstanding), I&#39;m wondering why I ever considered moving away from PHP. It&#39;s stable, reliable, and fails predictably all on top of me being well versed in how to write it and write it well. I&#39;m happy for the learning experience that&#39;s come out of all this and has probably helped make me a slightly better developer over all, but going where I&#39;ve never gone before on a &quot;massive&quot; project may not be the wisest decision after all. At the same time, I want to know if using this environment will make things faster, more scalable, with less overhead. Of course, Digg did this same thing in the summer of 2010.</p>
<p>And look at what happened to them...</p>
</div></article><article class="post"><header class="post__header"><h1 class="post__title"><a href="http://staticr.dxprog.com/entry/anime-reviews-2013-it-builds-character">Anime Reviews 2013 - It Builds Character</a></h1><time class="post__published">January 18, 2014</time></header><div class="post__content"><p>We&#39;re leisurely sauntering towards the finish line this year. Will we make it to the end? Maybe. But, as for today, here are the best/worst characters of 2013.</p>
<h2 id="best-characters">Best Characters</h2>
<p><img src="http://cdn.awwni.me/mzjz.jpg" alt="" title="The bokeh comes for free">
<strong>Chris - Yutaka Hasebe (Servant x Service)</strong>
When I first started watching Servant x Service, I honestly expected the characters to be set in stone in the first episode, staying as largely unchanged archetypes as the show progressed. This largely holds true for most of the characters, but there is an exception - Yutaka Hasebe.</p>
<p>His character starts out relatively one-dimensional, seemingly just a persistently happy, lazy bum with a penchant for collecting girls phone numbers. A pleasant enough bloke, as it stands. However, during the show&#39;s progression, you see that there is more to him than meets the eye, especially as you see him attempt to open up to Lucy about his feelings for her.</p>
<p>This blew my mind, as here we had a really fun, likable character with believable development, something that&#39;s almost unheard of in these sorts of slice of life shows. It really makes me want a second season just so I can see how he continues to develop as a character.[break]</p>
<p><img src="http://cdn.awwni.me/mzk3.jpg" alt="" title="Hyper spastic and ready to go!">
<strong>Jeff - Mako Mankanshoku (Kill la Kill)/Yutaka Hasebe (Servant x Service)</strong>
After several visions, and revisions, of this writing, I can find no better explanation for these two characters being the best than: They are my favorites. Sometimes I can write down my feelings, other times I can&#39;t. Negativity seems to be one I can always voice, though...</p>
<p><img src="http://cdn.awwni.me/mzk2.jpg" alt="" title="Them dark circles...">
<strong>Matt - Tomoko Kuroki (Watamote)</strong>
Tomoko is a broken person, there&#39;s no two ways about that. She&#39;s socially inept to the point that it might actually be diagnosable. But, there&#39;s a certain charm about her over the top delusions and awkwardness that keep her in the realm of likeable character. Her rampant imagination of how she predicts a social situation will play out versus her inability to actually say two words to a stranger, even a cashier, is pitiably lovable. Perhaps there&#39;s a bit of self-identification there; lord knows that I&#39;ve got my share of social anxieties from cenversating to my appearance. Tomoko, her awkward antics and highly cynical world view, get the knowing nod of approval and my vote for best character of last year.</p>
<h2 id="worst-character">Worst Character</h2>
<p><img src="http://cdn.awwni.me/mzk1.jpg" alt="" title="Looks are deceiving">
<strong>Chris - Kaga Kouko (Golden Time)</strong>
I like to think that I&#39;m pretty good at withholding judgement on characters until I&#39;ve had a few episodes to evaluate them, but boy shit did Kaga Kouko shatter that image. From her very first appearance in the very first episode I knew she was going to be a bitch deluxe.
However, that&#39;s not all. In addition to being the queen of bitches, she&#39; the needy and possessive empress of all bitches. This is really a problem, especially as far as the main character Tada Banri is concerned, given that not only does he put up with her fucking shit, he appears to be charmed by it. This blows my goddamn mind.
That such a relationship could be considered satisfying, let alone healthy, by any of the characters caught up in the whole shit storm indicates that anybody going along with it is clearly brain dead. This leads me to the hypothesis that Banri didn&#39;t lose his memories, he just lost his whole goddamn brain, and it&#39;s still sitting there under that fucking bridge, projecting it&#39;s angst in his general direction.
Oh shit, did I forget to mention that she’s also crazy rich, talentless, and completely entitled? I’m just glad Yana - her former &quot;boyfriend&quot; (read: possession) - had the balls to stand up and say, &quot;Bitch, you cray cray!&quot;
Perhaps that makes Yana the runner up for character of the year.</p>
<p><strong>Matt - Kaga Kouko (Golden Time)</strong>
You know the phrase &quot;don&#39;t stick your dick in crazy&quot;? Kaga Kouko, female lead in a series brought to us by the writer of Toradora!, is essentially the poster girl for the phrase. When she&#39;s first introduced, she&#39;s infatuated with her childhood friend and is not discrete about showing it. She&#39;s possessive, controlling, and has a jealous streak beyond all reason. Once said friend finally gets it through her thick skull that he hates her very existence, she replaces him with the show&#39;s lead, Tada Banri. During the brief transitional period, we find out that she&#39;s not really a bitch on purpose; she has a codependency problem larger than the Burj Khalifa. By her own admission, she simply cannot live unless she has a man to fawn over. That&#39;s all annoying in and of itself, but the fact that everybody enables her in this is what&#39;s really maddening. She&#39;s going to win the day (over better, stable, more personable girls) without any of her core problems being resolved when, in the end, all she really needs is a fuck ton of therapy. This show has only finished it&#39;s first cour, but I&#39;m not hopeful for the end or any redemption of Ms. Clingy Psychobitch.</p>
<p><img src="http://cdn.awwni.me/mzk0.jpg" alt="" title="If the eyes are the window to the soul, I don&#39;t want to see hers">
<strong>Jeff - Nase Izumi (Beyond the Boundary)</strong>
Typically, this spot is reserved for Kirino Kosaka. But she&#39;s such an awful character that she could get this every year and never be in danger of losing it, which makes for boring reading and is a tad cliche. So, in the spirit of keeping this different/fair, I went with Izumi Nase from&quot; Beyond the Boundary&quot;. Don&#39;t worry, she&#39;s still a bitch, just in different ways. She doesn&#39;t explain herself, ever, waltzs around following orders blindly for some douchebag in a wheelchair, and generally holds herself above everyone else in such a way as to come off as being elitist. And she&#39;s ice cold to everyone around her, including family. Spoiler alert: She put a hit on Akihito and treated him like he wasn&#39;t even a sentient being just because he had a youmu inside him. Guess what? More spoilers: she also has a youmu inside her, and she was aware of it the entire time. What a bitch.</p>
<p>That&#39;s all for today. Maybe we&#39;ll have the last installment up before the month is out. Maybe we won&#39;t...</p>
</div></article><article class="post"><header class="post__header"><h1 class="post__title"><a href="http://staticr.dxprog.com/entry/that-which-is-done-annually">That Which is Done Annually</a></h1><time class="post__published">January 3, 2014</time></header><div class="post__content"><p>As is per normal for me, it&#39;s time again to reexamine myself and birth a new Matt Hackmann with the new year. It&#39;s funny how this has become a thing because I used to be vehemently against &quot;New Year&#39;s Resolutions&quot;, taking that stance that a person should be able enact change at any time of the year. But, moreso than that, it&#39;s a barometer to measure how well those changes actually went. Anybody who knows a little bit about statistics knows that you need to monitor trends over time to get the most accurate data. All that said, let&#39;s see how I did against last year&#39;s list.</p>
<p><strong>Health</strong> - To be fair, I actually started out the year pretty well. I all but ceased fast food and was even getting in my daily walking. However, the move to California but a big fat wrench into all that. I write this have just downed half of a Papa John&#39;s pizza. There were some upsides, though, such as the fact that work is within a bikeable distance. Negating anything gained from that, though, was the big uptick in alcohol consumption. It&#39;s amazing how easy it is to drink when there are other people to drink with.</p>
<p><strong>Finances</strong> - Again, the job change had a unique impact on this category. The hard goals I had set out for myself were to reduce superfluous spending and eliminate my LASIK, credit card, and car debts. The middle two I actually accomplished and the car is within reach. However, if anything, the amount of money I dropped on a whim definitely increased, but because of the salary adjustment, I wasn&#39;t hurting as much for it.</p>
<p><strong>Learn Japanese</strong> - I bought the stuff and made absolutely no progress on this at all.</p>
<p><strong>Japan Trip</strong> - Mission accomplished with a repeat trip tentatively planned for 2015.</p>
<p><strong>Drawing</strong> - Pfffffft!</p>
<h2 id="the-2014-version">The 2014 Version</h2>
<p>I&#39;m going to shoot for simplicity this year, so here&#39;s the plan (that I will not follow):</p>
<p><strong>Eat Better</strong> - Again, fast food moratorium (as soon as my Taco Bell gift card is gone and the aforementioned pizza). It&#39;s easy enough to eat healthy at work (portion control being the most difficult part), but this needs to be carried over when I&#39;m not there. Going to have to compile a list of not-terrible-for-me-but-good-for-my-tastebuds recipes.</p>
<p><strong>Bike 150 miles every month</strong> - Once over the initial &quot;three week to develop a habit&quot; hump, this will actually be easy. My round trip work commute is twelve miles, so if I did that five days a week, that&#39;d actually be 200+ miles per month. I&#39;m giving a little bit of slack though. Start slow and all that.</p>
<p><strong>The Girlfriend Factor</strong> - I&#39;ve been single long enough. Time to push past my shyness and make that shit happen.</p>
<p>All in all, I&#39;d say two of those three things is attainable. We&#39;ll see what things are looking like when I write this post again next year.</p>
</div></article><div class="paging"><a href="http://staticr.dxprog.com/archives/11.html" class="paging__link paging__link--next">Earlier Posts</a><a href="http://staticr.dxprog.com/archives/9.html" class="paging__link paging__link--previous">Later Posts</a></div></section><footer class="footer"><p class="footer__copyright">Copyright © 2018 Matt Hackmann</p></footer></section></body></html>